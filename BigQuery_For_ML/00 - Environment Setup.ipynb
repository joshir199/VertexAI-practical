{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c69d4d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "![ga4](https://www.google-analytics.com/collect?v=2&tid=G-6VDTYWLKX6&cid=1&en=page_view&sid=1&dl=statmike%2Fvertex-ai-mlops%2F00+-+Setup&dt=00+-+Environment+Setup.ipynb)\n",
    "\n",
    "# 00 - Environment Setup\n",
    "\n",
    "This is the notebook that sets up the GCP project for the other notebooks in this repository.  Based on the [readme.md](../readme.md), you already have this repository of notebooks pulled as a local resource in your Vertex AI Workbench based notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee7178",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80aba5e",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c342aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up project Ids and region. \n",
    "REGION = 'us-central1'\n",
    "PROJECT_ID = 'caramel-park-375012'\n",
    "DATANAME = 'FRAUD_DETECTION'\n",
    "BUCKET = PROJECT_ID\n",
    "\n",
    "# setup data source table link in the notebook\n",
    "bq_source = 'bigquery-public-data.ml_datasets.ulb_fraud_detection'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632bae5a",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d470506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ade2ea",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Storage Bucket\n",
    "Create GCS storage bucket and also check to see if bucket already exist and create if missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "add618c7-2aef-45dd-8aaa-49330a382217",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs = storage.Client(project = PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa83e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Bucket: caramel-park-375012\n"
     ]
    }
   ],
   "source": [
    "if not gcs.lookup_bucket(BUCKET):\n",
    "    bucketDef = gcs.bucket(BUCKET)\n",
    "    bucket = gcs.create_bucket(bucketDef, project=PROJECT_ID, location=REGION)\n",
    "    print(f'Created Bucket: {gcs.lookup_bucket(BUCKET).name}')\n",
    "else:\n",
    "    bucketDef = gcs.bucket(BUCKET)\n",
    "    print(f'Bucket already exist: {bucketDef.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d707dc-98f4-4938-b5ec-2e3cf37681a8",
   "metadata": {},
   "source": [
    "---\n",
    "## Store project Data in created Storage Bucket\n",
    "After creating GCS storage bucket. Now, store the data from bigquery table to gcs storage as csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9365d0bb-2bb5-4e92-9341-b9b8a6021e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can directly add dataset in google cloud storage bucket through GCP page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7bef6",
   "metadata": {},
   "source": [
    "---\n",
    "## Install KFP\n",
    "If you get an error after a step, rerun it.  The dependecies sometimes resolve.\n",
    "- [Install the Kubeflow Pipelines SDK](https://www.kubeflow.org/docs/components/pipelines/v1/sdk/install-sdk/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx 0.26.4 requires absl-py<0.11,>=0.9, but you have absl-py 1.4.0 which is incompatible.\n",
      "tfx 0.26.4 requires attrs<21,>=19.3.0, but you have attrs 22.2.0 which is incompatible.\n",
      "tfx 0.26.4 requires click<8,>=7, but you have click 8.1.3 which is incompatible.\n",
      "tfx 0.26.4 requires docker<5,>=4.1, but you have docker 6.0.1 which is incompatible.\n",
      "tfx 0.26.4 requires jinja2<3,>=2.7.3, but you have jinja2 3.1.2 which is incompatible.\n",
      "tfx 0.26.4 requires kubernetes<12,>=10.0.1, but you have kubernetes 19.15.0 which is incompatible.\n",
      "tfx 0.26.4 requires pyarrow<0.18,>=0.17, but you have pyarrow 10.0.1 which is incompatible.\n",
      "tfx-bsl 0.26.1 requires absl-py<0.11,>=0.9, but you have absl-py 1.4.0 which is incompatible.\n",
      "tfx-bsl 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 10.0.1 which is incompatible.\n",
      "tensorflow 2.3.4 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.21.6 which is incompatible.\n",
      "tensorflow-transform 0.26.0 requires absl-py<0.11,>=0.9, but you have absl-py 1.4.0 which is incompatible.\n",
      "tensorflow-transform 0.26.0 requires pyarrow<0.18,>=0.17, but you have pyarrow 10.0.1 which is incompatible.\n",
      "tensorflow-model-analysis 0.26.0 requires absl-py<0.11,>=0.9, but you have absl-py 1.4.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.26.0 requires ipywidgets<8,>=7, but you have ipywidgets 8.0.4 which is incompatible.\n",
      "tensorflow-model-analysis 0.26.0 requires pyarrow<0.18,>=0.17, but you have pyarrow 10.0.1 which is incompatible.\n",
      "tensorflow-metadata 0.26.0 requires absl-py<0.11,>=0.9, but you have absl-py 1.4.0 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires absl-py<0.11,>=0.9, but you have absl-py 1.4.0 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires joblib<0.15,>=0.12, but you have joblib 1.2.0 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 10.0.1 which is incompatible.\n",
      "tensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorboard 2.3.0 requires google-auth<2,>=1.6.3, but you have google-auth 2.16.0 which is incompatible.\n",
      "tensorboard 2.3.0 requires google-auth-oauthlib<0.5,>=0.4.1, but you have google-auth-oauthlib 0.8.0 which is incompatible.\n",
      "ml-pipelines-sdk 0.26.4 requires absl-py<0.11,>=0.9, but you have absl-py 1.4.0 which is incompatible.\n",
      "ml-pipelines-sdk 0.26.4 requires docker<5,>=4.1, but you have docker 6.0.1 which is incompatible.\n",
      "ml-pipelines-sdk 0.26.4 requires jinja2<3,>=2.7.3, but you have jinja2 3.1.2 which is incompatible.\n",
      "ml-metadata 0.26.0 requires absl-py<0.11,>=0.9, but you have absl-py 1.4.0 which is incompatible.\n",
      "ml-metadata 0.26.0 requires attrs<21,>=20.3, but you have attrs 22.2.0 which is incompatible.\n",
      "jupyterlab-server 2.19.0 requires jsonschema>=4.17.3, but you have jsonschema 3.2.0 which is incompatible.\n",
      "google-cloud-aiplatform 1.21.0 requires packaging<22.0.0dev,>=14.3, but you have packaging 23.0 which is incompatible.\n",
      "apache-beam 2.28.0 requires httplib2<0.18.0,>=0.8, but you have httplib2 0.21.0 which is incompatible.\n",
      "apache-beam 2.28.0 requires numpy<1.20.0,>=1.14.3, but you have numpy 1.21.6 which is incompatible.\n",
      "apache-beam 2.28.0 requires pyarrow<3.0.0,>=0.15.1, but you have pyarrow 10.0.1 which is incompatible.\n",
      "apache-beam 2.28.0 requires typing-extensions<3.8.0,>=3.7.0, but you have typing-extensions 4.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install kfp -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cdd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing requirements for jsonschema: [Errno 2] No such file or directory: '/opt/conda/lib/python3.7/site-packages/jsonschema-4.17.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: No metadata found in /opt/conda/lib/python3.7/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Cannot uninstall jsonschema 4.17.3, RECORD file not found. You might be able to recover from this via: 'pip install --force-reinstall --no-deps jsonschema==4.17.3'.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-pipeline-components -U -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681525d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Update AIPlatform Package:\n",
    "\n",
    "The `google-cloud-aiplatform` package updates frequently.  Update it for latest functionality.\n",
    "\n",
    "- [aiplatform Python Client](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform)\n",
    "- [GitHub Repo for api-common-protos](https://github.com/googleapis/api-common-protos)\n",
    "\n",
    "For a better understanding of the Vertex AI APIs client, version, and layers please review the tip here [aiplatform_notes.md](../Tips/aiplatform_notes.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b910d824-e0ae-4547-ad41-048b73a0e83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing requirements for jsonschema: [Errno 2] No such file or directory: '/opt/conda/lib/python3.7/site-packages/jsonschema-4.17.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-aiplatform -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28e46217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing requirements for jsonschema: [Errno 2] No such file or directory: '/opt/conda/lib/python3.7/site-packages/jsonschema-4.17.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install plotly -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4299eac-ab92-4171-8f92-f5b633d1556a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing requirements for jsonschema: [Errno 2] No such file or directory: '/opt/conda/lib/python3.7/site-packages/jsonschema-4.17.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install googleapis-common-protos -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9c4c7cf-533d-40c1-9bbc-eb27bbad3fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = '01'\n",
    "SERIES = '01'\n",
    "BQ_TABLE = 'ulb_fraud_detection'\n",
    "\n",
    "file = f\"data/{BQ_TABLE}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ac36d53-96bd-4905-b1d3-04295a2c9ad8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.cloud.storage' has no attribute 'blobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_12157/823629680.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbucketDef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUCKET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mblb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucketDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'The file has already been created at: gs://{bucketDef.name}/{file}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'google.cloud.storage' has no attribute 'blobs'"
     ]
    }
   ],
   "source": [
    "bucketDef = gcs.bucket(BUCKET)\n",
    "for blb in storage.Blob(bucket=bucketDef):\n",
    "    print(blb)\n",
    "    print(f'The file has already been created at: gs://{bucketDef.name}/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234a103-ca90-4e26-b059-529c94973200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
